from transformers import GPT2Tokenizer, GPT2LMHeadModel

print("ðŸ”¹ Chargement du tokenizer et du modÃ¨le...")

tokenizer = GPT2Tokenizer.from_pretrained("sshleifer/tiny-gpt2")

# âš¡ Utiliser safetensors pour Ã©viter l'erreur torch < 2.6
model = GPT2LMHeadModel.from_pretrained(
    "sshleifer/tiny-gpt2",
    use_safetensors=True  # <- clÃ© ici
)

print("âœ… ModÃ¨le et tokenizer chargÃ©s")

prompt = "Le diabÃ¨te est une maladie"
inputs = tokenizer(prompt, return_tensors="pt")

print("ðŸ”¹ Lancement de la gÃ©nÃ©ration (CPU rapide)...")

outputs = model.generate(
    **inputs,
    max_length=50,
    do_sample=True,
    temperature=0.7,
    top_k=50
)

print("ðŸ©º RÃ©ponse du modÃ¨le :")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
