Medical Text Generation with Tiny-GPT2
Ce projet consiste à collecter des résumés médicaux depuis PubMed, à les nettoyer, puis à fine-tuner un modèle de langage Tiny-GPT2 afin de générer automatiquement du texte médical (descriptions, résumés, phrases explicatives).
L’objectif principal est l’expérimentation en NLP médical, l’apprentissage du fine-tuning de modèles de langage et la compréhension du pipeline complet Data → Modèle → Génération.

 Fonctionnalités
 
 Récupération automatique de résumés médicaux depuis PubMed
 Nettoyage et préparation du texte médical
 Fine-tuning du modèle Tiny-GPT2 (Hugging Face)
 Génération de texte médical à partir d’un prompt
 Entraînement léger sur CPU (pas besoin de GPU)

Technologies utilisées

Python
PyTorch
Hugging Face Transformers
PubMed API
Tiny-GPT2

 Cas d’usage
 
Recherche et expérimentation en NLP médical
Génération de données synthétiques
Apprentissage du fine-tuning de modèles de langage
Projet pédagogique en intelligence artificielle

Avertissement

Ce projet est à but éducatif et expérimental.
Les textes générés ne doivent en aucun cas être utilisés comme avis médical.
